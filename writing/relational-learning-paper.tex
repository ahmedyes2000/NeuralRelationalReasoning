\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts0
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

%=====================================================================
%============================ our packages ===========================

\usepackage{color}
\definecolor{black}{rgb}{0,0,0}
\usepackage[colorlinks, linkcolor=black, urlcolor=black, citecolor=black]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,positioning, fit, arrows.meta, shapes}
\usetikzlibrary{shapes}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{float}

%=====================================================================
%============================= citations =============================

% Flexible sec/fig/tbl/def cross-refs.
\newcommand{\Secref}[1]{Sec.~\ref{#1}}
\newcommand{\secref}[1]{sec.~\ref{#1}}
\newcommand{\dashsecref}[2]{secs.~\ref{#1}--\ref{#2}}

\newcommand{\Defref}[1]{Definition~\ref{#1}}
\newcommand{\defref}[1]{Definition~\ref{#1}}
\newcommand{\Defrefc}[2]{\Defref{#1}, clause~\ref{#2}}
\newcommand{\defrefc}[2]{\defref{#1}, clause~\ref{#2}}

\newcommand{\Figref}[1]{Figure~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\dashfigref}[2]{Figures~\ref{#1}--\ref{#2}}
\newcommand{\Tabref}[1]{Table~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

\newcommand{\Appref}[1]{Appendix~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}

% In-text citations
\newcommand{\posscitet}[1]{\citeauthor{#1}'s~(\citeyear{#1})}
\newcommand{\sposscitet}[1]{\citeauthor{#1}'~(\citeyear{#1})}
\newcommand{\possciteauthor}[1]{\citeauthor{#1}'s}
\newcommand{\spossciteauthor}[1]{\citeauthor{#1}'}
\newcommand{\pgposscitet}[2]{\citeauthor{#1}'s~(\citeyear{#1}:~#2)}
\newcommand{\secposscitet}[2]{\citeauthor{#1}'s~(\citeyear{#1}:~$\S$#2)}
\newcommand{\pgcitealt}[2]{\citealt{#1}:~#2}
\newcommand{\seccitealt}[2]{\citealt{#1}:~$\S$#2}
\newcommand{\pgcitep}[2]{(\citealt{#1}:~#2)}
\newcommand{\seccitep}[2]{(\citealt{#1}:~$\S$#2)}
\newcommand{\pgcitet}[2]{\citeauthor{#1}~(\citeyear{#1}:~#2)}
\newcommand{\seccitet}[2]{\citeauthor{#1}~(\citeyear{#1}:~$\S$#2)}

% Examples:
\newcommand{\eg}[1]{(\ref{#1})}
\newcommand{\dasheg}[2]{\eg{#1}--\eg{#2}}
\newcommand{\subeg}[2]{(\ref{#1}\ref{#2})}
\newcommand{\dblsubeg}[3]{(\ref{#1}\ref{#2},~\ref{#3})}
\newcommand{\dashsubeg}[3]{(\ref{#1}\ref{#2}--\ref{#3})}

%=====================================================================
%========================= font manipulation =========================

\newcommand{\word}[1]{\emph{#1}}
\newcommand{\tech}[1]{\emph{#1}}
\newcommand{\highlight}[1]{\textbf{#1}}

\newcommand{\blue}[1]{{\color{blue}#1}}

\definecolor{ourgreen}{HTML}{4D8951}
\definecolor{darkblue}{HTML}{0499CC}
\definecolor{superlightgray}{HTML}{B8B8B8}

\newcommand{\poscell}[1]{\fcolorbox{white}{ourgreen}{#1}}
\newcommand{\negcell}[1]{\fcolorbox{white}{superlightgray}{#1}}

\newcommand{\update}[1]{{\color{darkblue}#1}}

%=====================================================================
%================================ math ===============================

\newcommand{\softmax}{\mathbf{softmax}}
\DeclareMathOperator{\ReLU}{ReLU}
\newcommand{\identity}{\mathbb{I}}
\newcommand{\LSTM}{\textbf{LSTM}}
\newcommand{\wupdate}{\mathrel{{+}{=}}}

%=====================================================================
%=====================================================================

\title{Relational reasoning and generalization using non-symbolic neural networks}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Atticus Geiger \\
  Department of Linguistics\\
  Stanford University \\
  \And
  Alexandra Carstensen \\
  Department of Psychology \\
  Stanford University \\
  \AND
  Michael C.~Frank \\
  Department of Psychology \\
  Stanford University \\
  \And
  Christopher Potts \\
  Department of Linguistics\\
  Stanford University
}


\begin{document}

\maketitle

\begin{abstract}
\update{%
  Humans have a remarkable capacity to reason about abstract relational structures, an ability that may support some of the most impressive, human-unique cognitive feats. Because equality (or identity) is a simple and ubiquitous relational operator, equality reasoning has been a key case study for the broader question of abstract relational reasoning. This paper revisits the question of whether equality can be learned in non-symbolic neural networks. Drawing on recent developments in the study of such networks, we assess out-of-sample generalization of equality using both arbitrary representations and arbitrary representations that have been pretrained on separate tasks to imbue them with abstract structure. In this setting, we find that even simple neural networks are able to learn basic equality with relatively little training data. In a second case study, we show that sequential equality problems (learning ABA sequences) can be solved with only positive training instances. Finally, we consider a more complex, hierarchical equality problem, but this requires vastly more data. However, we show that using a pretrained equality network as a modular component of this larger task leads to good performance with no task-specific training. Overall, these findings indicate that neural models are able to solve equality-based reasoning tasks, suggesting that essential aspects of symbolic reasoning can emerge from data-driven, non-symbolic learning processes.%
}
\end{abstract}


\section{Introduction}\label{sec:introduction}

One of the key components of human intelligence is our ability to reason about abstract relations between stimuli. Many of the most unremarkable human activities -- scheduling a meeting, following traffic signs, assembling furniture -- require a fluency with abstraction and relational reasoning that is unmatched in nonhuman animals. An influential perspective on human uniqueness holds that relational concepts are critical to higher-order cognition \citep[e.g.,][]{Gentner:2003}. By far the most common case study of abstract relations has been equality.\footnote{We use the term ``equality'' here, though different literatures have also used ``identity.''} Equality is a valuable case study because it is simple and ubiquitous, but also completely abstract in the sense that it can be evaluated regardless of the identity of the stimuli being judged.

Equality reasoning has been studied extensively across a host of systems and tasks, with wildly variant conclusions. In some studies, equality is very challenging to learn: only great apes with either extensive language experience or specialized training succeed in matching tasks in which a \emph{same} pair, AA, must be matched to a novel same pair, BB \citep{Premack:1983,thompson:2001}. Preschool children struggle to learn the same regularities in a seemingly similar task \citep{walker:2016}. In contrast, other studies suggest that equality is simple: bees are able to learn abstract identity relationships from only a small set of training trials \citep{avargues:2011}, and infants as young as 3 months can generalize an identity pattern \citep{anderson:2018}. We take the central challenge of this literature to be characterizing the conditions that lead to success or failure in learning an abstract relation in a way that can be productively generalized.

The learning task in all of these cases can be described by the predicate \emph{same} (or equivalently, =).  However, this symbolic vocabulary does not provide a lever to distinguish which of these tasks are trivial and which are immensely difficult. What would an explanation of this gradient look like? Ideally, we would want a model or set of models that describe under what conditions \emph{same} is easy and under what conditions it is hard or unlearnable -- and how learning proceeds in these hard cases. One perspective in the literature is that this type of learning task requires symbolic representations \citep{marcus:1999,Premack:1983} -- but this view does not fully explain many of the cases on the table. For instance, explanations of newborn identity detection under this type of account are circular \citep{gervain:2012}: because newborns succeed, they then must have symbolic representations. Does this logic apply also to bees?


\subsection{Models of equality learning}

Addressing the empirical puzzles of equality learning requires a theoretical framework beyond the symbolic/non-symbolic distinction. To make quantitative predictions about task performance, such a framework should ideally be instantiated in a computational model that takes in training data and learns a solution that generalizes when assessed with stimuli analogous to those used in experimental assessments. A range of models of this type have been studied \citep{alhama:2019}. Symbolic models of generalization (e.g., \citealt{frank:2011}) can be used to make contact with data, but they presuppose the existence of a symbolic equality predicate and hence simply presuppose symbolic abilities in every case of success -- an unsatisfying conclusion.

In contrast, neural network models provide a framework for arbitrary function learning that might in principle be used to understand the emergence of equality relations. Contra this proposal, however, \citet{marcus:1999} argued that a broad class of recurrent neural networks were unable to learn equality relations. \citeauthor{marcus:1999}'s claims were subsequently challenged by \citet{dienes:1999}, \citet{seidenberg:1999a}, \citet{seidenberg:1999b}, \citet{elman:1999}, and \citet{negishi:1999}, all of whom presented evidence that neural networks are able to learn (at least aspects of) the tasks that \citeauthor{marcus:1999} posed. The subsequent debate \citep[reviewed in][]{alhama:2019} revealed a striking lack of consensus on some of the ground rules regarding both (1) what representations would qualify as non-symbolic and (2) what sort of generalization would be required to show that the learned function was suitably abstract. We review these issues below; in brief, we adopt (1) completely random distributed representations and (2) fully disjoint training and test vocabularies, with the goal of solving very challenging generalization tasks without building any relevant structure into our basic representations.


\subsection{Representations}\label{sec:representations}

\newcommand{\mysquare}{
  \begin{tikzpicture}[scale=0.5]
    \node[rectangle, fill=red!100, minimum height=3mm, minimum width=3mm]{};
  \end{tikzpicture}}

\newcommand{\myrectangle}{
  \begin{tikzpicture}[scale=0.25]
    \node[rectangle, fill=blue!100, minimum height=3mm, minimum width=5mm]{};
  \end{tikzpicture}}

\newcommand{\mypentagon}{
  \begin{tikzpicture}[scale=0.5]
    \node[regular polygon,regular polygon sides=5, minimum height=4mm, fill=red!100]{};
  \end{tikzpicture}}


\newcommand{\mypolygon}{
  \begin{tikzpicture}[scale=0.5]
    \node[regular polygon,regular polygon sides=7, minimum height=4mm, fill=blue!100]{};
  \end{tikzpicture}}


\begin{figure*}[t!]
  \setlength{\arraycolsep}{2pt}
  \centering
  \small
  \begin{subfigure}[t]{0.05\textwidth}
    \centering
    \begin{tabular}[b]{@{} c @{}}
      \midrule
      \mysquare \\
      \myrectangle \\
      \mypentagon \\
      $\vdots$ \\
      \mypolygon \\
      \bottomrule
    \end{tabular}
    \caption{}
    \label{fig:entities}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.2\textwidth}
    \centering
    \renewcommand{\arraystretch}{1.24}
    $\begin{array}[b]{*{5}{c}}
       \toprule
       \mysquare & \myrectangle & \mypentagon & &  \mypolygon \\
       \midrule
       1 & 0 & 0 & \cdots & 0 \\
       0 & 1 & 0 & \cdots & 0 \\
       0 & 0 & 1 & \cdots & 0 \\
       \multicolumn{5}{c}{\vdots} \\
       0 & 0 & 0 & \cdots & 1 \\
       \bottomrule
    \end{array}$
    \caption{Localist.}
    \label{fig:reps:localist}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.14\textwidth}
    \centering
    \renewcommand{\arraystretch}{1.24}
    $\begin{array}[b]{*{4}{r}}
       \toprule
       \rotatebox{90}{red} & \rotatebox{90}{blue} & \rotatebox{90}{sides} & \rotatebox{90}{green} \\
       \midrule
       1 & 0 & 4 & 0  \\
       0 & 1 & 4 & 0  \\
       1 & 0 & 5 & 0  \\
       \multicolumn{4}{c}{\vdots} \\
       0 & 1 & 7 & 0  \\
       \bottomrule
    \end{array}$
    \caption{Distributed.}
    \label{fig:reps:symbolic}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.29\textwidth}
    \centering
    \renewcommand{\arraystretch}{1.24}
    $\begin{array}[b]{rrrrr}
       \toprule
       d_{1} &  d_{2} &  d_{3} &  d_{4} &  d_{5} \\
       \midrule
       0.1 &   -0.5 &   -0.2 &   -0.3 &    0.2 \\
       0.2 &    0.4 &   -0.4 &   -0.1 &   -0.5 \\
       -0.3 &    0.0 &   -0.5 &   -0.3 &    0.1 \\
       \multicolumn{5}{c}{\vdots} \\
       0.0 &   -0.3 &    0.1 &    0.3 &   -0.5 \\
       \bottomrule
    \end{array}$
    \caption{Random.}
    \label{fig:reps:random}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.29\textwidth}
    \centering
    \renewcommand{\arraystretch}{1.24}
    $\begin{array}[b]{rrrrr}
       \toprule
       d_{1} &  d_{2} &  d_{3} &  d_{4} &  d_{5} \\
       \midrule
        0.4 &    0.2 &   -0.5 &   -1.1 &   -0.3 \\
       0.6 &    0.7 &    0.7 &    0.7 &    0.5 \\
       0.8 &   -0.2 &    0.1 &   -0.0 &    0.5 \\
       \multicolumn{5}{c}{\vdots} \\
       0.3 &   -1.4 &    0.1 &    0.4 &   -0.7 \\
       \bottomrule
    \end{array}$
    \caption{Pretrained.}
    \label{fig:reps:pretrained}
  \end{subfigure}

  \vspace{12pt}

  \begin{subfigure}[t]{0.48\linewidth}
    \includegraphics[width=1\linewidth]{../fig/toy-example-original.pdf}
    \caption{The matrix in \figref{fig:reps:random} (before pretraining).}
    \label{fig:tsne-random}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \includegraphics[width=1\linewidth]{../fig/toy-example-trained.pdf}
    \caption{The matrix in \figref{fig:reps:pretrained} (after pretraining).}
    \label{fig:tsne-pretrained}
  \end{subfigure}

  \caption{\update{Representations options for the notional entities in \figref{fig:entities}.}}
  \label{fig:reps}
\end{figure*}

Representation schemes for neural network models can be placed in \update{four} broad classes. At one end of the spectrum, we have \tech{localist representation} schemes, as in \figref{fig:reps:localist}. Here, each row represents an entity, and every entity is a vector with a single $1$ at its own column dimension, and in turn every column has exactly one $1$ in it. There is no shared structure across objects; all are equally (un)related to each other as far as the model is concerned. These representations are a symbolic system in which all the symbols are meaningless unique identifiers.

In the middle of the spectrum, we find \tech{distributed feature representations}, as in \figref{fig:reps:symbolic}. Here, we build in more structure by designing a rich feature space. In this setting, the entities can have rich relationships to each other, as encoded in the shared structure given by the columns. We can again think of these representations as symbolic, but we would do this via the columns, by naming each column and then considering the entities to be analyzed as various values of these features, and two entities would be labeled as identical if they had exactly the same values for all features.

At the other end of the spectrum, we have \tech{completely random feature representations}, as in \figref{fig:reps:random}. Here, each entity is represented by a unique vector of random real-valued numbers. As with localist encoding, this defines all the entities as equally (un)related to each other, since column-wise patterns are unlikely and, to the extent that they are present, they exist completely by chance.

\update{
In between distributed and random representations are \tech{pretrained} representations. These are typically obtained by starting with random representations and imbuing them with rich structure via a learning process. \Figref{fig:reps:pretrained} provides a simple example. This matrix is the results of pretraining the representations in \figref{fig:reps:random} against three independent binary classification objectives. (\Appref{app:pretraining} provides technical details on our pretraining approach.) Superficially, the two matrices looks equally random. However, this is only because the pretraining process leads to very abstract kinds of structure. Intuitively, individual features can be encoded in multiple dimensions of these spaces, making it hard to discern. We can bring out this latent structure using visualizations such as those in \figref{fig:tsne-random} and \figref{fig:tsne-pretrained}, which uses the t-SNE algorithm \citep{vanderMaaten:Hinton:2008} to depict an approximation of the original high-dimensional matrices in two dimensions. We use color to distinguish the different label combinations determined by the three tasks. At left, we have the representations in \figref{fig:reps:random}, and at right we have the pretrained representations. The visualization reveals that pretraining has created structure akin to what we see transparently in distributed representations.

Pretraining need not be restricted to input representations. In principle, entire networks can be pretrained to solve specific tasks. This more general form of pretraining has the potential to find latent associations even between completely random inputs, since such networks can learn parameters that define intermediate representations with potentially as much structure as one would encode by hand in a featural representation. Such pretraining offers the possibility that networks might be used as modular components to solve more complex tasks.
}

Localist representations are transparently symbolic and useless in disjoint generalization tasks. In \appref{app:generalization}, we show analytically that localist representations cannot lead to out-of-sample generalization. In contrast, distributed feature representations can be useful in such tasks, but they require grounding entities in property domains such as color, shape, or sound. Thus, such featural representations both require specialized domain knowledge and (arguably) build in symbolic structure \citep{marcus:1999a}. We avoid these issues here by representing all entities using completely random distributed representations. This allows us to provide general results on relational learning, independent from the domains that entities are grounded in, and it forces our models to learn in a completely unstructured space. \update{In addition, we report results for pretrained representations, showing that they consistently lead to even faster learning.}


\subsection{Generalization}

The standard approach to training and evaluating neural networks is to choose a dataset, divide it randomly into training and assessment sets, train the system on the training set, and then use its performance on the assessment set as a proxy for its capacity to generalize to new data.

The standard approach is fine for many purposes, but it raises concerns in a context in which we are trying to determine whether a network has truly acquired a global solution to a target function.  In particular, where there is any kind of overlap between the training and assessment vocabularies (primitive elements), we can't rule out that the network might be primarily taking advantage of idiosyncrasies in the underlying dataset to effectively cheat -- to memorize aspects of the training set and learn a local approximation of the target function that happens to provide traction during assessment.

To address this issue, we propose that networks must be evaluated on assessment sets that are completely disjoint in every respect from the train set, all the way down to the entities involved. For example, below, we train on pairs $(a, a)$ and $(a, b)$, where $a$ and $b$ are representations from a train vocabulary $V_{T}$. At test time, we create a new assessment vocabulary $V_{A}$, derive equality and inequality pairs $(\alpha, \alpha)$ and $(\alpha, \beta)$ from that vocabulary, and assess the trained network on these new examples. \update{Where pretrained inputs are used, $V_{T}$ and $V_{A}$ must come from the same representation space, but we still define  $V_{T}$ and $V_{A}$ using completely different vocabularies. In adopting these methods,} we get a clear picture of the system's capacity to generalize, and we can safely say that its performance during assessment is a window into whether a global solution to identity has been learned. \update{This is a very challenging setting for any machine learning model.  In the language modeling context, we will see that it even requires us to departure from usual model formulations.}


\subsection{The current paper}

\begin{figure}[tp]
  \centering
  \includegraphics[scale=0.20]{modelfigv1.pdf}
  \caption{Relational reasoning tasks. Green and red mark positive and negative training examples, respectively. The sequential task (Model~2) uses only positive instances, and a model succeeds if, prompted with $\alpha$, it produces a sequence $\beta \ \alpha$ for $\beta \neq \alpha$. For the hierarchical task (Model~3), we show that a model trained on the basic task (Model~1) is effective with no additional training.}
  \label{fig:tasks}
\end{figure}

This paper revisits the debate about the capacity of neural networks to solve relational reasoning tasks in ways that resemble human behavior. We consider three cases of identity-based reasoning that have featured prominently in discussions of the role of symbols in relational reasoning (\figref{fig:tasks}): (1) learning to discriminate pairs of objects that exemplify the relation \emph{same} or \emph{different}, (2) learning sequences with repeated \emph{same} elements, as in \citet{marcus:1999}, and (3) learning to distinguish hierarchical \emph{same} and \emph{different} relations in a context with pairs of pairs exemplifying these relations, as in \citet{Premack:1983}.

To preview our results, our central finding is that even simple neural networks are effective at the tasks we pose\update{, and that pretraining the input representations generally leads to faster learning}.  The sequence learning task shows additionally that explicit negative evidence is not always needed. However, for hierarchical equality, an unrealistic amount of training data is required. To address this concern, we consider the \update{extended pretraining regimes mentioned in \secref{sec:representations}, in which a network trained for equality is used as a modular component in a larger network.} Pretraining greatly reduces the data demands of our models, even leading to success with no additional training in the hierarchical task.

Overall, these findings indicate that neural models are able to solve equality-based reasoning tasks, suggesting that essential aspects of symbolic reasoning can emerge from entirely data-driven, non-symbolic learning processes.


\section{Model 1: Same--different relations}\label{sec:equality}

In our first model, we consider whether a supervised, feed-forward classification model can learn equality (and inequality) relations in the strict setting we describe above: random representations with disjoint generalization examples.


\subsection{Model}

Our basic model for equality is a feed-forward neural network with a single hidden representation layer:
%
\begin{align}
  h &= \ReLU([a;b]W_{xh} + b_{h}) \label{eq:x2h}\\
  y &= \softmax(hW_{hy} + b_{y}) \label{eq:h2y}
\end{align}
%
The input is a pair of vectors $(a, b)$, each of dimension $m$, which correspond to the two stimulus objects. These vectors are random distributed representations that do not have features encoding properties of the objects or their identity. These are concatenated to form a single vector $[a;b]$ of dimension $2m$, which is the simplest way of merging the two representations to form a single input.

This representation is multiplied by a matrix of weights $W_{xh}$ of dimension $2m \times n$ and a bias vector $b_{h}$ of dimension $n$ is added to this result, where $n$ is the hidden layer dimensionality. These two steps create a linear projection of the input representation, and the bias term is the value of this linear projection when the input representation is the zero vector. Then, the non-linear activation function $\ReLU$ ($\ReLU(x) = \max(0, x)$) is applied element-wise to this linear projection. This non-linearity is what gives the neural model more expressive power than a logistic regression. The result is the hidden representation $h$.

The hidden representation is the input to the classification layer: $h$ is multiplied by a second matrix of weights $W_{hy}$, dimension $n \times 2$, and a bias term $b_{y}$ (dimension 2) is added to this. This second bias term encodes the probabilities of each class when the hidden representation is 0. The result is fed through the softmax activation function: $\softmax(x)_{i} = \frac{\exp{x_{i}}}{\sum_{j} \exp{x_{j}}}$.  This creates a probability distribution over the classes (positive and negative). For a given input, the model computes this probability distribution and the input is categorized as the class with the higher probability.

During training, this model is presented with positive and negative labeled examples and the parameters $W_{xh}$, $W_{hy}$, $b_{y}$, and $b_{h}$ are learned using back propagation with a cross entropy loss function. \update{This function is defined as follows, for a corpus of $N$ examples and $K$ classes:
%
\begin{equation}
  \max(\theta)
  \quad
  \frac{1}{N}
  \sum_{i=1}^{N}
  \sum_{k=1}^{K}
  y^{i,k} \log(h_{\theta}(i)^{k})
\end{equation}
%
where $\theta$ abbreviates the model parameters ($W_{xh}$, $W_{hy}$, $b_{y}$, $b_{h}$), $y^{i,k}$ is the actual label for example $i$ and class $k$, and $h_{\theta}(i)^{k}$ is the corresponding prediction.}

During testing, this model is tasked with categorizing inputs unseen during training.  It is straightforward to show that a network like this is capable of learning equality as we have defined it. \Appref{app:equality-solution} provides an analytic solution to the equality relation using this neural model.  \update{Here we illustrate with a small example network that maps all identity pairs to $[0.5, 0.5]$ and all non-identity pairs to $[y, 1-y]$ where $y > 0.5$, which supports a trivial classification rule:
%
\setlength{\arraycolsep}{4pt}
\begin{equation}
  \ReLU\left(
    [a;b]
    \left(
      \begin{array}[c]{@{} *{4}{r} @{}}
        1 &  0 &  -1 & 0  \\
        0 &  1 &  0 & -1 \\
        -1 &  0 &  1 & 0  \\
        0 &  -1 &  0 & 1
      \end{array}
    \right)
    +
    \mathbf{0}
  \right)
  \left(
    \begin{array}[c]{@{} r r @{}}
      1 & 0 \\
      1 & 0\\
      1 & 0\\
      1 & 0\\
    \end{array}
  \right)
  +
  \mathbf{0}
\end{equation}}

\begin{figure*}[tp]
  \centering
  \begin{subfigure}[t]{0.48\linewidth}
    \includegraphics[width=1\linewidth]{../fig/equality-train_size-embed_dim-hidden_dim=100.pdf}
    \vspace{-4mm}
    \caption{Results for a model in which the hidden layer has dimensionality $100$. The lines correspond to different dimensions for the entities $a$ and $b$ in the input pairs $(a, b)$. These models largely pass 90\% accuracy with 1,000 training examples and reach (near) perfection by 1,250.}
    \label{fig:equality--smallresults}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{../fig/equality-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
    \caption{\update{Results for pretraining on different numbers of tasks. The `no pretraining' model is the best of the models at left (10-dimensional embeddings, 100-dimensional hidden layers), repeated to faculitate comparisons. The pretraining models use those same dimensionalities.}}
    \label{fig:basic-equality-pretrain}
  \end{subfigure}
  \caption{Same--different results with and without pretraining.}
\end{figure*}


This result shows that equality in our sense is learnable in principle, but it doesn't resolve the question of whether networks can find this kind of solution given finite training data. To address this, we train the network on a stream of pairs of random vectors. Half of these are identity pairs $(a, a)$, labeled with $1$, and half are non-identity pairs $(a, b)$, labeled with $0$. Trained networks are assessed on the same kind of balanced dataset, with vectors that were never seen in training so that, as discussed earlier, we get a clear picture of whether they have found a generalizable solution.  Further optimization details are in \appref{app:optimization}.


\subsection{Results}

\Figref{fig:equality--smallresults} presents typical results. This is for the case where the hidden layer dimensionality is $100$, and we plot results for different embedding dimensionalities $m$ and different amounts of training data. The picture is comparable with hidden dimensionalities at 10, 25, and 50, but those models require more training data to reach (near) perfect performance (\appref{app:model1-results}).


\subsection{Discussion}

Our assessment pairs have nothing in common with the training pairs except insofar as both involve vectors of real numbers of the same dimensionality. During training, the network is told (via labels) which pairs are equality pairs and which are not, but the pairs themselves contain no information about equality per se. It thus seems fair to us to say that these networks have learned equality, or at least how to simulate that relation with near perfect accuracy.

\update{%
  While all the networks in \figref{fig:equality--smallresults} reach above-chance performance almost immediately, they require upwards of 1,000 examples to truly solve these tasks. To try to speed up learning, we ran the above experiments using pretrained representations as well, holding out one of the pretraining classes as our test set as described above. \Figref{fig:basic-equality-pretrain} summarizes the results of these experiments for 10-dimensional embeddings and 100-dimensional hidden representations, as this seems to be the network that learns the fastest with random inputs. We see a clear speed-up, with more pretraining tasks resulting in the largest gains.}


\section{Model 2: Sequential same--different (ABA task)}\label{sec:lms}

Our first model is simple and successfully learns equality. However, this model was supervised with both positive and negative evidence. In the initial debate around these issues, supervision with negative evidence was dismissed as an unreasonably strong learning regime \citep[e.g.,][]{marcus:1999a}. While this argument likely holds true for language learning \citep[in which supervision is generally agreed not to be direct;][]{brown:1970}, it is not necessarily true for learning more generally. Nevertheless, learning of sequential ABA rules without negative feedback is possible for infants \citep{marcus:1999,rabagliati:2019}. Our next model explores whether neural network models can learn this task in a challenging regime with no negative supervision.


\subsection{Model}

To explore learning with only positive instances, we adopt a neural language model.  The hope is that these models can learn the underlying principles that govern their inputs, for the purposes of generating new sequences or discriminating among different kinds of input.

Neural language models are sequential models. At each timestep, they predict an output given their predictions about the preceding timesteps. As typically formulated, the prediction function is just a classifier: at each timestep, it predicts a probability distribution over the entire vocabulary of options, and the item with the highest probability is chosen as a symbolic output. This becomes the input at the next timestep, and the process continues.

This formulation will not work in situations in which we want to make predictions using an entirely different vocabulary. The classifier function will get no feedback about these out-of-vocabulary items during training, and so it will never predict them during assessment.  To address this issue, we need to reformulate the prediction function. Our proposal is to have the model predict output vector representations -- instead of discrete vocabulary items -- at each timestep.  During training, the model is trained to minimize the distance between these output predictions and the representations of the actual output entities. During assessment, we take the prediction to be the item in the entire vocabulary (training and assessment) whose representation is closest to the predicted vector (in terms of Euclidean distance). This fuzzy approach to prediction creates enough space for the model to predict sequences from an entirely new vocabulary.

The specific model we use for this is as follows:
%
\begin{align}
  h_{t} &= \LSTM(x_{t}, h_{t-1}) \label{eq:lstm-recur}\\
  y_{t} &= h_{t}W + b\label{eq:lstm-predict}
\end{align}
%
This holds for $t > 0$, and we set $h_{0} = \mathbf{0}$. $\LSTM$ is a long short-term memory cell \citep{hochreiter:1997}. Full details on these cells are given in \secref{sec:analyticlm}.

The input is a sequence of vectors $x_1, x_2, x_3, \dots$, each of dimension $m$, which correspond to a sequence of stimulus objects.  These vectors are, again, random distributed representations that do not have features encoding properties of the objects or their identity.


At each timestep $t$, the input vector $x_t$ is fed into the $\LSTM$ cell along with the previous hidden representation $h_{t-1}$. The defining feature of an $\LSTM$ is the ability to decide whether to store information from the current input, $x_t$, and whether to remember or forget the information from the previous timestep $h_{t-t}$. The output of the $\LSTM$ cell is the hidden representation for the current time step $h_t$. The dimension of the hidden representations is $n$. The hidden representation is multiplied by a matrix $W$ with dimensionality $n \times m$ to produce $y_t$. This result, $y_t$, is a linear projection of the hidden representation into the input vector space, which is necessary because $y_t$ is a prediction of what the next input, $x_{t+1}$, will be.

\update{
  The objective function is as follows:
%
\begin{equation}
  \max(\theta)
  \quad
  \frac{1}{N}
  \sum_{i=1}^{N}
  \sum_{t=1}^{T_{i}}
  \left\| h_{\theta}\left(x^{i, 0:{t-1}}\right) - x^{i,t} \right\|^{2}
\end{equation}
%
for $N$ examples. Here, $T_{i}$ is the length of example $i$. As before, $\theta$ abbreviates the parameters of the model as specified in \dasheg{eq:lstm-recur}{eq:lstm-predict}. We use $h_{\theta}(x^{i, 0:{t-1}})$ for the vector predicted by the model for example $i$ at timestep $t$, which is compared to the actual vector at timestep $t$ via squared Euclidean distance (i.e., the mean squared error).

As noted above, this is an unusual formulation for a language model. The usual version essentially treats every timestep as involving a classification decision, with a cross-entropy loss. We cannot adopt this because of our goal of using unseen vocabulary items at test time.}


\Appref{sec:analyticlm} provides an analytic solution to the ABA task using this model.  To see how well it does in practice, we trained networks on sequences \texttt{<s> a b a </s>}, where $\texttt{b} \neq \texttt{a}$. We show the network every such sequence during training, from an underlying vocabulary of 20 items (creating a total of 380 examples). To assess how well the model learns this pattern, we seed it with \texttt{<s> x} where \texttt{x} is an item from a disjoint vocabulary from that seen in training, and we say that a prediction is accurate if the model continues with \texttt{y x </s>}, where \texttt{y} is any character (from the training or assessment vocabulary) except \texttt{x}.


\subsection{Results}

\Figref{fig:fuzzy-lm-results} shows the results for a model with a 100-dimensional hidden representation. As before, the results are comparable for smaller networks. Unlike for the previous equality experiments, we found that we had to allow the model to experience multiple epochs of training on the same set in order to succeed, and hence the x-axis counts examples in terms of epochs of learning on our fixed set of 380 examples.

\begin{figure*}[tp]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{../fig/fuzzy-lm-vocab20-train_size-embed_dim-hidden_dim=100.pdf}
    \caption{Results for a model in which each $h_{t}$ in Eq.~\eg{eq:lstm-recur} has dimensionality 100. The lines correspond to the dimensionality of the input representations ($x_t$ in Eq.~\eg{eq:lstm-recur}). All the training examples are presented at once over multiple epochs.}
    \label{fig:fuzzy-lm-results}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{../fig/fuzzy-lm-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
    \caption{\update{Results for pretraining on different numbers of tasks. The `no pretraining' model is the best of the models at left in terms of speed and overall performance (2-dimensional embeddings, 100-dimensional hidden representations). The pretraining models use those same dimensionalities. Pretraining does not lead to a noticeable change in speed or accuracy for this task.}}
    \label{fig:fuzzy-lm-pretrain-results}
  \end{subfigure}
  \caption{Sequential same--different results with and without pretraining.}
\end{figure*}


\subsection{Discussion}

These models succeed at learning the underlying patterns in our data, even though they use only random representations.  They are given no negative instances, and they must predict into a totally new vocabulary. \update{The shortcoming we see here is that the learning process is admittedly slow and data-intensive. We hypothesized that pretraining leads to noticeable speed-ups, as it did in \secref{sec:equality}, but we did not see this in practice;  \figref{fig:fuzzy-lm-pretrain-results} shows that pretraining does not help with learning.} However, we speculate that there may be variants of our model that reduce these demands, given that learning is in principle possible in this architecture.


\section{Model 3: Hierarchical same--different relations}\label{sec:premack}

Given the strong results for equality, we can ask whether more challenging equality problems are also learnable in our setting. An interesting test-case in this regard is the hierarchical equality task used by \citet{Premack:1983}: given a pairs of pairs $((a,b), (c,d))$, the label is $1$ if $(a = b) = (c = d)$, else $0$.  \citet{Premack:1983} suggested that the ability exemplified by this task -- reasoning about hierarchical \emph{same} and \emph{different} relations -- could represent a form of symbolic abstraction uniquely enabled by language.  Given the non-symbolic nature of our models, our simulations provide a test of this hypothesis, though we should look critically at their ability to find good solutions with reasonable amounts of training data.


\subsection{Model}

We can approach this task using the same model and methods as we used for equality. The only change required to equations \dasheg{eq:x2h}{eq:h2y} is that we create inputs $[a;b;c;d]$: the flat concatenation of all the elements of the two pair of vectors. This in turn leads $W_{xh}$ to have dimensionality $4m \times n$.

These models are able to find nearly perfect solutions, but vastly more training data is required for this task than was required for simple equality, and the network configuration matters much more. For example, our model with 10-dimensional entity representations and 100-dimensional hidden representations reaches near perfect accuracy, but only with over 95,000 training instances, and a comparable model with 50-dimensional entity representations failed to get traction at all with this amount of training data. \update{\Appref{app:model1-premack} provides a full picture of thee learning trends. Pretraining does lead to faster and more stable learning, but the improvements are relatively minor.}

We hypothesized that the flat input representations $[a;b;c;d]$ might be suboptimal here. This task is intuitively hierarchical: if one works out the equality labels for each of the two pairs, then the further classification decision can be done entirely on that basis. Our current neural network might be too shallow to find this kind of decomposition.  To address this, we can simply add another intermediate layer:
%
\begin{align}
  h_{1} &= \ReLU([a;b;c;d]W_{xh} + b_{h_{1}}) \label{eq:x2h1}\\
  h_{2} &= \ReLU(h_{1}W_{hh} + b_{h_{2}}) \label{eq:x2h2}\\
  y &= \softmax(h_{2}W_{hy} + b_{y}) \label{eq:h2y2}
\end{align}


\subsection{Results}

\update{\Figref{fig:premack-h2-flat-results} shows that} these deeper networks learn faster and are more robust to different network configurations than our single-layer variant. Intuitively, these models are better structured to find hierarchical solutions to this hierarchical problem.

\begin{figure*}[tp]
  \centering
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
   \includegraphics[width=1\linewidth]{../fig/flatpremack-h2-train_size-embed_dim-hidden_dim=100.pdf}
   \caption{Results for a network with two 100-dimensional hidden layers. Nearly all the networks solve the task, but they require very large training sets to do so.}
   \label{fig:premack-h2-flat-results}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{../fig/flatpremack-h2-pretrain-compare-train_size-pretrained-embed_dim=None.pdf}
    \caption{\update{Results for pretraining on different numbers of tasks. The `no pretraining' model is the best of the models att left in terms of speed (10-dimensional embeddings, 100-dimensional hidden representations). It is repeated for comparison, but we show only up to 30,000 examples to reveal more detail in the learning trends. Pretaining greatly increases the speed of learning.}}
    \label{fig:premack-h2-pretrain}
  \end{subfigure}
  \caption{\update{Hierarchical same--different results with and without pretraining.}}
\end{figure*}


\subsection{Discussion}

These results are encouraging, but we still require more than 20,000 training instances to reach top performance. \update{As \figref{fig:premack-h2-pretrain} shows, pretraining helps substantially with this shortcoming, but we still require upwrds of 10,000 examples to fully solve the problem. This is still vastly more data than human participants get in similar experiments.}  Thus, it is worth asking whether there are other solutions that would be more data efficient and thus more in line with human capabilities.  In the next section, we seek to capitalize even more on the hierarchical nature of this task by defining a modular pretraining regime in which previously learned capabilities are recruited for new tasks.


\subsection{The critical role of experience}\label{sec:modular}

Our successful results training neural networks on simple equality suggest another strategy for solving the hierarchical equality task. Rather than requiring our networks to find solutions from scratch, we can pretrain them to do basic equality and then use those parameters as a starting point for learning hierarchical equality. \update{This is conceptually similar to our previous experiments with pretraining, but now we seek to pretrain an entire subpart of the model, rather than just its inputs.}


\subsubsection{Model}

The hierarchical equality task requires computing the equality relation three times: compute whether the first two inputs are equal, compute whether the second two inputs are equal, then compute whether the truth value outputs of these first two computations are equal. We propose to use the same network pretrained on basic equality to perform all three equality computations.  More precisely, we define
%
\begin{align}
  h_1 &= \ReLU([a;b]W_{xh} + b_{h})\\
  h_2 &= \ReLU([c;d]W_{xh} + b_{h})\\
  h_3 &= \ReLU([h_1;h_2]W_{xh} + b_{h}) \\
  y &= \softmax(h_3W_{hy} + b_{y})
\end{align}
%
where $W_{xh}$, $W_{hy}$, $b_h$, and $b_y$ are the parameters from the model in equations \dasheg{eq:x2h}{eq:h2y} already trained on basic equality.  Crucially, the same parameters, $W_{xh}$ and $b_h$, are used three times: twice to compute representations encoding whether a pair of input entities are equal ($h_1$, $h_2$), and once to compute a representation ($h_{3}$) encoding whether the truth values encoded by $h_1$ and $h_2$ are equal. This final representation is then used to compute a probability distribution over two classes, and the class with the higher probability is predicted by the model.

\begin{figure*}[tp]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
  \includegraphics[width=1\linewidth]{../fig/input-as-output-train_size-embed_dim-hidden_dim=0.pdf}
  \caption{Basic equality networks applied to the hierarchical equality task. Each line represents a different embedding dimensionality, which is constrained in this model to match the hidden dimensionality. Even with no additional training instances for this task, all models achieve greater than chance accuracy, and even modest amounts of additional training on the task lead to excellent performance.
  }
  \label{fig:premack-pretraining-results}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{../fig/input-as-output-zero-shot.pdf}
    \caption{\update{The results from the plot at left where the x-axis is 0, that is, where we are testing zero-shot generalization from the pretrained equality network to the hierarchical setting. All the models perform well above chance in this setting, with the 50-dimensioanl version achieving a mean of 67\% accuracy.}}
    \label{fig:premack-pretraining-zero-shot-results}
  \end{subfigure}
  \caption{Modular network results for the hierarchical same--different task.}
  \label{fig:modular-results}
\end{figure*}


\subsubsection{Results and discussion}

\update{\Figref{fig:modular-results} shows that this model succeeds very quickly at this task. As we see in \figref{fig:premack-pretraining-results}, after just a few thousand examples, the majority of the model configurations perform with near perfect accuracy. The findings in \figref{fig:premack-pretraining-zero-shot-results} are even more striking: all the models have above chance performance after being trained only on the simple equality task -- that is, they achieve zero-shot generalization to the hierarchical task.} It is remarkable that a model trained only on equality between entities is able to get traction on a problem that requires determining whether equality holds between the truth values encoded in two learned representations.

\update{
It might be possible to effectively combine network pretraining with input pretraining as in the previous experiments. An initial exploration of this idea is presented in \appref{app:double-pretain}. While we have not yet found a way to use this combination of pretraining regimes to improve over \figref{fig:modular-results}, we are optimistic about such combinations.}


\section{General discussion}

We presented a series of neural network models for learning and generalizing equality relations, showing that these non-symbolic models succeed even in stringent assessments, and, in the case of neural language models, even without being shown negative examples. We support these results with analytic insights into why networks are able to find solutions to these tasks.

In some settings, our current models require more training instances than humans seem to need. However, our pretraining approach to the hierarchical equality task suggests a path forward: by using pretrained models as modular components, we can get traction on challenging tasks without any training specifically for those tasks, and even a small amount of additional training can be transformative.  We hypothesize that these modular pretrained components might serve as the basis for even more complex cognitive abilities.


\section*{Acknowledgements}

\update{This work is supported in part by a Facebook Robust Deep Learning for Natural Language Processing Research Award.}


\bibliographystyle{acl_natbib}
\bibliography{relational-learning-bib}

\input{relational-learning-supplement.tex}

\end{document}
